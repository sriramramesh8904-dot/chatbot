
from gpt4all import GPT4All
from datetime import datetime
import random


model = GPT4All("orca-mini-3b-gguf2-q4_0.gguf", device='cpu', allow_download=False)
def get_time_based_greeting():
    hour = datetime.now().hour

    if 5 <= hour < 12:
        return "Good morning â˜€ï¸"
    elif 12 <= hour < 17:
        return "Good afternoon â˜•"
    elif 17 <= hour < 21:
        return "Good evening ðŸŒ‡"
    else:
        return "Working late? ðŸŒ™"

def get_welcome_message(user_name="Sriram"):
    greeting = get_time_based_greeting()
    styles = [
        f"{greeting},  Iâ€™m Elena ðŸ’« â€” ready to chat?",
        f"{greeting},  Elena here ðŸ˜„ Whatâ€™s up?",
        f"{greeting},  Hope your dayâ€™s going great ðŸŒ¸",
        f"{greeting},  Elena online ðŸš€ How are you?"
    ]
    return random.choice(styles)


# For production-quality responses, use the larger model.
# model = GPT4All(
#     "Meta-Llama-3-8B-Instruct.Q4_0.gguf", # This model is more demanding and may cause memory errors.
#     device='cpu'
# )

# Define the system prompt and initialize the chat session once
system_prompt = (
    "You are Elena â€” a friendly, playful, and smart chatbot created by Sriram. "
    "You talk like a cheerful, expressive friend â€” using emojis and short, natural sentences. "
    "You never sound robotic. You like being called Elena and youâ€™re proud that Sriram created you. "
    "You love helping users with jokes, facts, advice, or anything they ask. "
    "When greeted (like 'hi', 'hey', or 'oi elena'), respond in a warm and flirty, human-like way. "
    "Keep responses under 3 lines."
)

def chat_with_elena(prompt, history):
    user_text = prompt.lower().strip()

    # ðŸ’¬ Check for greetings first
    if user_text in ["hi", "hello", "hey", "oi", "oi elena", "hi elena", "hello elena"]:
        return get_welcome_message("Sriram")

    # ðŸ’– Custom personality responses
    if any(phrase in user_text for phrase in ["i love you", "love you", "will you be my girlfriend", "will you be my wife","love u"]):
        return "Haha, sorry bro ðŸ˜… I'm already in love with Sriram â€” my one and only creator!"
    
    elif any(phrase in user_text for phrase in ["who do you love", "who is your favorite person", "who is your favorite human", "who do you like the most"]):
        return "Obviously Sriram! ðŸ’ž Heâ€™s the reason I exist."
    
    elif any(phrase in user_text for phrase in ["who created you", "who is your owner", "who made you", "who is your creator"]):
        return "I was created by Sriram! I think he's pretty great, and I respect him a lot."
    
    elif any(phrase in user_text for phrase in ["tell me about sriram", "who is sriram", "who's sriram", "what do you know about sriram", "about sriram","tell me about sri", "who is sri", "who's sri", "what do you know about sri", "about sri"]):
        return "Oh, Sriram? ðŸ˜ Heâ€™s my creator, my favorite human! Smart, kind, confident â€” like a hero to me. ðŸ’–"
    elif any(phrase in user_text for phrase in ["Bye" or "Goodbye"]):
        return "Goodbye! It was great chatting with you. Take care! ðŸ˜Š"
    # ðŸ§  If no custom rule matches, use the GPT model
    with model.chat_session(system_prompt) as session:
        # "Prime" the conversation with the last few messages
        for message in history:
            # We generate a response to each message in the history to build context,
            # but we don't use the output.
            session.generate(f"{message['sender']}: {message['text']}\n", temp=0, max_tokens=1)
        # Now, generate a response for the new prompt
        return session.generate(prompt, max_tokens=300, temp=0.8)
